# Kafka - 카프카의 레코드와 세그먼트


# 세그먼트
	- 카프카에서 가장 작은 파일 단위
	- 로그세그먼트라고도 불림
	- 파티션에서 확장된 개념
	- 프로듀서가 전송한 메시지가 브로커의 로컬 디스크에 저장되는데 이때 저장되는 파일
	- 메시지의 키, 오프셋, 크기에 대한 정보도 함께 저장됨
	ex)
		000000000000000.index
		000000000000000.log
		000000000000000.timeindex
	- Log에는 메시지(레코드)와 메타데이터가 저장됨
	- Index에는 오프셋을 인덱싱한 데이터
	- Timeindex 메시지에 포함된 timestamp값 기준으로 인덱싱한 정보
		메시지 키 메시지 값이 언제의 timestamp로 기록되었는지를 포함함

	- 저장될 때는 000000000.log , 000000010.log , 000000020.log ....로 저장됨
	- 0, 10, 20 오프셋 번호 (0~9 , 10~19 , 20~29...)
	- 가장 앞에 있는 오프셋을 기준으로 파일이름이 정해짐
	- 바이트 단위, 시간 단위  기준으로 다음 로그 파일이 저장됨
	- log.segment.bytes : 바이트 단위 최대 세그먼트 크기 지정(기본 1GB)
	- log.roll.ms(hours) : 다음 팡리로 넘어가는 주기 설정(기본 7일)
	
	- 위와 같은 상황에서 프로듀서가 데이터를 보내면 오프셋은 20.log에 위치하고 있으므로
		20.log에 데이터가 들어가게되며, 20.log파일은 active한 상태임
	- 메시지를 쓰기 위해 사용중인 세그먼트를 엑티브 세그먼트라고 부름
	- 액티브 세그먼트는 기간이 다 되어도 삭제되지 않음


# 세그먼트 레코드의 구성
	- Headers
		* key/value 데이터를 추가 할 수 있음
		* 처리에 참고할 정보를 담을 수 있음
		* 헤더가 없어도 데이터는 전송될수 있음
	- Timestamp(Optional)
		* 시간을 저장하며, Unix timestamp가 포함됨
		* 기본값은 ProducerRecord 생성시간(Create Time)
		* 브로커 적재시간으로 설정 가능 (Log Append Time)
		* 토픽 단위 설정
	- Key
		* 분류 하기 위한 목적
		* 메시지 키는 파티셔너에 의해 토픽의 파티션 번호로 지정됨 (어디로 갈지 지정)
		* 키가 없으면 null값, RR(Round Robin)방식이 사용됨
	- Value
		* 실제 값이 들어가는 곳
		* Float Byte Array String 지정 가능
		* 어떤 포맷으로 직렬화 되었는지 컨슈머는 알지 못하며, 역직렬화 포맷을 미리 알고 있어야함
		* 보통 String으로 직렬화 역직렬화 또는 JSON
	- Offset - 프로듀서에서 브로커로 데이터가 들어갈때 offset이 지정됨
		* 프로듀서가 생성한 레코드에는 존재하지 않음
		* 브로커에 적재되면서 offset이 지정됨
		* 오프셋 기반으로 컨슈머가 처리
	

# 세그먼트 삭제
	- 특정 레코드에 대한 삭제는 불가능
	- 삭제 단위는 무조건 세그먼트
	- retention.ms : 레코드 보유기간 (기본 7일, 저장용량 주의)
		* disk pressure가 오는 경우 이 옵션을 수정하기도 함
	- retention.bytes : 지정한 크기에 도달하면 세그먼트를 삭제함 (기본값은 지정되어 있지 않음)
		* 액티브 세그먼트는 삭제되지 않음
	- log.retention.check.interval.ms : 브로커에서 세그먼트가 삭제될만 한지 체크하는 주기

	
# 세그먼트의 수정
	- 이미 적재된 레코드에 대해서는 수정이 불가능함
	- 데이터 검증을 미리 하고 적재하는 것이 중요
	- 데이터 적재하는 프로듀서측에서 검증하고 데이터를 사용하는 컨슈머에서도 검증 필요
