

## 실전 프로젝트 - 배치

- 지난 강의들을 종합해보는 프로젝트
- 가상 시나리오를 설정하고 배운 내용들을 통해 문제를 해결해보자



## 배치 시나리오

> 여러분은 **Fast Jake Company**의 **Data Enineer**입니다.
>
> 타 팀 요청을 처리한지 얼마지나지 않아 또 다시 요청이 왔습니다.
>
> "이전에 넣어주신 데이터를 기반으로 **다양한 분석**을 해서 월 리포트를 작성하려고하는데 어떻게 해야할까요?"
>
> 여러분에게 주어진 과제는 **Elasticsearch**에 적재된 데이터를 **쉽게 분석**할 수 있도록 만드는 것입니다.

- 요청 내용을 살펴보니 적재된 외부 데이터를 분석하기 쉽도록 만드는 것입니다.
- 배운 내용을 복기하면서 어떻게 쉽게 분석할 수 있게 만들 수 있을지, 이를 위해서 어떻게 연동을 할지 생각해 봅시다.



## 배치 파이프라인 설계

![batch-pipeline](image/batch-pipeline.png)

- 월 마다의 리포트 작성을 위해 실시간으로 데이터를 연동해야할 필요는 없습니다.
  - 실시간으로 분석을 해야할 요건은 없는 것으로 보입니다.
- 배치 파이프라인으로 Airflow를 사용할 수 있을 것 같습니다.
  - 적재된 Elasticsearch에서 데이터를 가져와 적재할 수 있습니다.
  - S3에 미리 Elasticsearch에 적재되는 데이터를 넣어두었습니다.
- 쉽게 분석할 수 있게 만드는 것으로 SQL을 생각할 수 있습니다.
  - SQL을 사용해 월 리포트를 위한 데이터를 만들 수 있도록 합니다.
  - 대표적인 DB로 MySQL을 사용할 수 있습니다.
    - (Data Warehouse를 분석계로 많이 사용하지만 실습비용 문제로 Redshift는 사용하지 않겠습니다)
- S3 -> Airflow -> MySQL 로 데이터 파이프라인을 구성하고 분석환경을 구축할 수 있습니다.



### 데이터 인입 확인

- ![api-pipeline](image/api-pipeline.png)
- 기존에 설정해 놓은 upbit-api 데이터가 Kafka로 제대로 인입이 되고 있는지 확인
- UI For Kafka 확인
- Logstash 확인
  - S3경로에 파일이 생성되고 있는지 확인



### Airflow Setting

- airflow main, airflow db, airflow worker 서버 시작

- github repository를 clone해 DAG를 작성
  - `git clone https://github.com/jake-fc/fastcampus-airflow.git`

- provider 설치 확인

  ```undefined
  pip install apache-airflow-providers-mysql
  ```

  - provider가 설치되어야 **connection Type** **MySQL** 사용가능

  - https://airflow.apache.org/docs/apache-airflow-providers/

    

- DAG 작성

  - ```python
    from datetime import datetime, timedelta
    from airflow import DAG
    from airflow.providers.amazon.aws.hooks.s3 import S3Hook
    from airflow.providers.mysql.operators.mysql import MySqlOperator
    from airflow.providers.mysql.hooks.mysql import MySqlHook
    from airflow.operators.python_operator import PythonOperator
    
    from datetime import datetime, timedelta
    
    import pandas as pd
    import io
    import json
    
    default_args = {
        'owner': 'jake',
        'depends_on_past': False,
        'start_date': datetime(2023, 4, 10),
        'retries': 0,
        'retry_delay': timedelta(minutes=5),
    }
    
    dag = DAG(
        's3_to_mysql_batch',
        default_args=default_args,
        schedule_interval='0 * * * *',
        catchup=True
    )
    
    s3_bucket_name = 'jake-api'
    mysql_table_name = 'upbit_api'
    mysql_conn_id = 'mysql_conn'
    s3_conn_id = 'aws_default'
    
        
    def insert_s3_data_bulk(execution_date, **context):
        ds_nodash = execution_date.strftime('%Y%m%d%H')
        s3_hook = S3Hook(aws_conn_id=s3_conn_id)
        s3_prefix = f'api/upbit-api/year={ds_nodash[:4]}/month={ds_nodash[4:6]}/day={ds_nodash[6:8]}/hour={ds_nodash[8:10]}/'
        s3_files = s3_hook.list_keys(bucket_name=s3_bucket_name, prefix = s3_prefix)
        
        print(f'Today is {ds_nodash}')
    
        print(s3_prefix)
        print(s3_files)
        
        json_lists = []
        cleaned_s3_files = []
        for s3_file in s3_files :
            if "year=2023" in s3_file:
                cleaned_s3_files.append(s3_file)
    
        print("Done 1")
    
        for cleaned_s3_file in cleaned_s3_files:
            s3_object = s3_hook.get_key(cleaned_s3_file, s3_bucket_name)
            s3_file = s3_object.get()['Body'].read().decode('utf-8')
            json_lists += [json.loads(json_str) for json_str in s3_file.strip().split('\n')]
    
        print("Done 2")    
        
        del_columns = ["s3_bucket","idx_name","s3_path","@version","@timestamp"]
        for json_list in json_lists:
            for del_column in del_columns:
                json_list.pop(del_column)
    
        print("Done 3")
    
        for json_list in json_lists:
            df = pd.DataFrame.from_dict([json_list])
            mysql_hook = MySqlHook(mysql_conn_id=mysql_conn_id)
            mysql_hook.insert_rows(table=mysql_table_name, replace=True, rows=df.values.tolist())
    
    with dag:
        # S3 파일을 MySQL에 적재하는 PythonOperator
        load_s3_file_to_mysql_task = PythonOperator(
            task_id='load_s3_file_to_mysql',
            python_callable=insert_s3_data_bulk,
            queue="celery",
            provide_context=True
        )
    ```
    
    
  
- *S3 전체 데이터 Insert DAG

  - ```python
    from datetime import datetime, timedelta
    from airflow import DAG
    from airflow.providers.amazon.aws.hooks.s3 import S3Hook
    from airflow.providers.mysql.operators.mysql import MySqlOperator
    from airflow.providers.mysql.hooks.mysql import MySqlHook
    from airflow.operators.python_operator import PythonOperator
    import pandas as pd
    import io
    import json
    default_args = {
        'owner': 'jake',
        'depends_on_past': False,
        'start_date': datetime(2023, 4, 21),
        'retries': 0,
        'retry_delay': timedelta(minutes=5),
    }
    dag = DAG(
        's3_to_mysql_v2',
        default_args=default_args,
        schedule_interval=timedelta(days=1),
    )
    s3_bucket_name = 'jake-api'
    mysql_table_name = 'upbit_api'
    mysql_conn_id = 'mysql_conn'
    s3_conn_id = 'aws_default'
    
    def insert_s3_data_bulk(**context):
        s3_hook = S3Hook(aws_conn_id=s3_conn_id)
        s3_files = s3_hook.list_keys(s3_bucket_name)
        
    
    
        json_lists = []
        cleaned_s3_files = []
        for s3_file in s3_files :
            if "year=2023" in s3_file:
                cleaned_s3_files.append(s3_file)
        print("Done 1")
        for cleaned_s3_file in cleaned_s3_files:
            s3_object = s3_hook.get_key(cleaned_s3_file, s3_bucket_name)
            s3_file = s3_object.get()['Body'].read().decode('utf-8')
            json_lists += [json.loads(json_str) for json_str in s3_file.strip().split('\n')]
        print("Done 2")    
        
        del_columns = ["s3_bucket","idx_name","s3_path","@version","@timestamp"]
        for json_list in json_lists:
            for del_column in del_columns:
                json_list.pop(del_column)
        print("Done 3")
        for json_list in json_lists:
            df = pd.DataFrame.from_dict([json_list])
            mysql_hook = MySqlHook(mysql_conn_id=mysql_conn_id)
            mysql_hook.insert_rows(table=mysql_table_name, replace=True, rows=df.values.tolist())
    with dag:
        # S3 파일을 MySQL에 적재하는 PythonOperator
        load_s3_file_to_mysql_task = PythonOperator(
            task_id='load_s3_file_to_mysql',
            python_callable=insert_s3_data_bulk,
            queue="celery",
            provide_context=True
        )
    
    insert_s3_data_bulk
    ```

    

- 워커와 main 서버에 필요한 패키지 필요

  ```shell
  pip install airflow.providers.amazon
  pip install pandas
  ```

- WebUI에서 로그가 제대로 보이지 않는다면, airflow.cfg에서 `secret_key`를 확인
  - airflow main서버와 airflow worker의 secret_key가 다르면 제대로 fetch가 되지 않는다.



### RDS 생성 (MySQL)

- AWS에서 RDS를 생성

- RDS 생성 후에 스키마, 테이블 생성

  - ```mysql
    CREATE SCHEMA api;
    ```

  - ```mysql
    CREATE TABLE `upbit_api` (
      `trade_volume` float DEFAULT NULL,
      `trade_timestamp` bigint DEFAULT NULL,
      `change` varchar(10) DEFAULT NULL,
      `trade_time_kst` varchar(20) DEFAULT NULL,
      `signed_change_rate` float DEFAULT NULL,
      `signed_change_price` float DEFAULT NULL,
      `lowest_52_week_price` float DEFAULT NULL,
      `high_price` bigint DEFAULT NULL,
      `opening_price` bigint DEFAULT NULL,
      `highest_52_week_date` varchar(20) DEFAULT NULL,
      `acc_trade_price_24h` float DEFAULT NULL,
      `acc_trade_price` float DEFAULT NULL,
      `highest_52_week_price` float DEFAULT NULL,
      `trade_date` varchar(20) DEFAULT NULL,
      `acc_trade_volume` float DEFAULT NULL,
      `prev_closing_price` float DEFAULT NULL,
      `change_price` float DEFAULT NULL,
      `trade_date_kst` varchar(20) DEFAULT NULL,
      `trade_price` bigint DEFAULT NULL,
      `change_rate` float DEFAULT NULL,
      `trade_time` varchar(20) DEFAULT NULL,
      `acc_trade_volume_24h` float DEFAULT NULL,
      `lowest_52_week_date` varchar(20) DEFAULT NULL,
      `pipeline_id` varchar(20) DEFAULT NULL,
      `timestamp` bigint NOT NULL,
      `market` varchar(10) DEFAULT NULL,
      `low_price` bigint NOT NULL,
      PRIMARY KEY (`timestamp`)
    ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb3;
  ```

- Airflow에 Connectinos - mysql_conn 설정



### Airflow Worker 오토 스케일링

- 워커의 리소스가 모자르게 되면 OOM등의 문제가 발생할 수 있음
- 워커는 실행불가능 상태에 빠지게 되어 작업처리가 늦어지거나, DAG 실행이 Fail되어버림
- 오토스케일링 적용을 통해 작업을 분산처리 한다.



### 주의사항

- Connections 설정
- MySQL에 Column과 airflow에서 넣을 데이터의 Column이 일치해야 함
  - Column개수는 물론이고 순서도 일치해야 한다.
- DAG에 Catchup 옵션을 True로 했을 때
  - airflow의 리소스를 모니터링 해야함
  - 많은 수의 Task가 실행될 것이기 때문에 airflow의 리소스가 빠르게 고갈됨(특히 워커)

- UTC, KST 시간 확인 필요